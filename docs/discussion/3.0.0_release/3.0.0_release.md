# Overall Design Goals

* Ease of use: allow for shorthands, *without* tying it to validation
* Speed: should be competitive with native encoding/decoding operations, *at least for non-complex operations*
* Memory light: will help with speed as well
    - Complex metadata associated with blueprints should be opt-in, not always on
* Linting: the ability to inspect blueprints with varying degrees of scrutiny, including none at all; deliver helpful error and warning messages
    - Opt in
    - Version robust: accurate across temporal version changes
* Modularity: the ability to turn on and off features depending on your use case

## Validation

`blueprint.validate()`

What does the above actually mean? Does it mean validate everything inside of the blueprint (entities, tiles, schedules, etc.) Or does it mean just validate the keys of blueprint, and you have to explicitly call:

```py
blueprint.entities.validate()
blueprint.tiles.validate()
blueprint.schedules.validate()
# etc...
```

Furthermore, this gets more complex as we consider even deeper nesting like `BlueprintBook`s:

```py
blueprint_book = BlueprintBook("...")

blueprint_book.validate()
# Does this validate just the top level attributes?
# Does it validate all contained blueprints?
# Does it validate all contained blueprints, and recurse through any contained blueprint books and validate their blueprints?
```

Answer: `blueprint.validate()` should be recursive, in that it should validate *every* component of `blueprint`. 

If you want to validate just one part of `blueprint`, maybe each attribute should have a `validate()` member:

```py
blueprint.label.validate().reissue_all()
```

This is good because it would prevent users from having to validate the *entire* blueprint if they just want to check one or few attributes.

What about when you want to check more than just a few? Say we want to validate all root attributes of `blueprint`. We have two options:

```py
blueprint.validate(recursive=False).reissue_all()
# or
result = ValidationResult([], [])
for attribute in blueprint.attributes:
    result += attribute.validate().reissue_all()
```

What does `blueprint.entities.validate()` do?

Either one is valid, which means whatever solution should be able to do either one depending on the situation.

Validating assignment is ideal in some circumstances and not in others. Sometimes we want the extra layer of user friendliness:

Other times we really don't want this behavior, particularly when we're doing something fast:

## Should a blueprint be validated on construction?

Answer: probably not. Maybe its better to just do something like:

```py
# First create the blueprint, coercing wherever possible
blueprint = Blueprint("...", if_unknown="accept")
# Then actually validate/lint the blueprint, giving explainations why it's busted
blueprint.validate().reissue_all()
```

The problem is how rigorously do we define the coercion of the first step? And what should Draftsman do if it's given a blueprint which is uninterpretable?

* If a key is required but not provided, set it to a default value
    - Issue an error of some kind
* If a key is given but not required
    - Issue a warning of some kind

## The conundrum of `tiles` in `Group`

Do we want to do something like `blueprint.content?`

Probably not; I don't want to lose the ability to access just entities and tiles, since this is how it is internally anyway.

Would be bad in a number of ways. However, what if we had a generic interface for group-like objects?

```py
group = Group()

blueprint.append(group)

assert group.entities in blueprint.entities
assert group.tiles in blueprint.tiles

# but how would you access group to say change its position when inside of blueprint?

blueprint.groups
```

I'm considering relaxing the distinction between Blueprint and Group. There are slight differences (Blueprints can have icons and metadata and stuff while Groups can have positions) but in practice they might not be much different.

```py
blueprint = Blueprint("some string")
blueprint.position = (10, 10)

blueprint2 = Blueprint()
blueprint2.add(blueprint)
```


# Permissive Construction, Strict Validation

```py
# What should `entity` be here?
entity = new_entity("my-modded-entity")
# If we assert that new_entity always returns an `Entity` class with the maximum amount specified...

# If we want the "accept" behavior, we're already there
# If we want the "ignore" behavior, we simply just check if it is a known entity:
entity = None if entity.name not in entities.raw else entity
# If we want a "error and warning" behavior, we just validate the entity:
entity.validate(mode="strict").reissue_all()
# If we want a custom error behavior, we repurpose the ignore logic:
if entity.name not in entities.raw:
    raise Exception("Unrecognized entity!")

# This is far better than having a `if_unknown` keyword, because it moves it out of the 
# hidden backend and into the forefront where it matters. This is especially important
# because this kind of behavior is incredibly user specific, so putting it in their code
# improves readability.

# Furthermore, this means we can apply this property to blueprintables:

# Load a blueprint with if_unknown = "accept"
blueprint = Blueprint("modded blueprint string")

# If we want to omit any modded entities, just do this:
for entity in blueprint.entities:
    if entity.name not in entities.raw:
        blueprint.entities.remove(entity)
# Or if you want to be fancy we could maybe use `filter` (at the cost of maybe some performance):
blueprint.entities = filter(lambda entity: entity.name in entities.raw, blueprint.entities)

# Errors and warnings:
blueprint.validate(mode="pedantic") # Error: unknown entity "my-modded-entity"

# Custom error:
unknown_entities = list(filter(lambda entity: entity.name not in entities.raw, blueprint.entities))
if not unknown_entities.empty():
    raise Exception("I thought I got rid of you all!")
```

This makes absolute sense, I'm removing `if_unknown`.

* Entities should issue `OverlappingEntityWarnings` themselves, accessing their parent via their attribute. This means that both adding a new entity to a blueprint will raise these warnings as well as validating the entire blueprint at once (since `Blueprint.validate()` will iterate over every sub-entity and call `validate()` on each of them)

I want attributes to be their original types, so `type(blueprint.version)` will be `int`, `type(blueprint.icons[0])` will be `dict`, etc. This is to keep continuity with the underlying root format and keep things straightforward.

This does mean that we have two copies of the data: The `Pydantic` model with its validation, and then the `_root` dict containing the modifiable data. For exporting we just output the `_root` dict, and for validation we just convert data into a model and validate it that way.

* Ideally, attributes of Draftsman objects should behave equivalently to their JSON dict counterparts (such as attribute access `blueprint["attribute"]["subattribute"]`)
* Attribute access across Draftsman ideally should be consistent. `blueprint["attribute"]["subattribute"]` and `blueprint.attribute.subattribute` should be interchangeable syntaxes.
* If a parent object has a truthy `validate_assignment`, then subattributes should also inherit this behavior. For example, `blueprint.label_color.a = "wrong"` should issue a `DataFormatError` when strict. *This should happen only when setting by attribute, in order to provide different use cases for item and attribute setting*.
    - As far as I can tell, this is impossible traditionally; you would need two different copies of the same model and swap between them depending on the configuring attribute, or manually wrap `__setattr__` with custom code, which is clunky and a pain in the ass.
* Problem: I want to be able to specify JSON representations as attributes
    - If we go the Pydantic instance route, we need to:
    - 1. Normalize shorthand formats
    - 2. Convert the input into a Pydantic model (if input format dictates so)
    - 3. Optionally validate the assignment, depending on the value of `validate_assignment`

All of this is only a problem if we want validation dynamically via attribute setting. If we only perform validation when the user explicitly asks for it, then

However, having the feature of item safety is good and a reason to use Draftsman in the first place.

I think the best course of action will be to create a good enough solution for now, and have the validation apply to only the root most attribute (not recursively like it should). This will catch 99% of use cases, while at least laying the groundwork for future improvements (perhaps when a more flexible validator is used)


```py
# 1
blueprint.entities.validate_assignment = "strict" # I don't like the extra state implied here; this could be hidden as fuck if you don't know about it
blueprint.entities.append("unknown")

# vs

# 2
blueprint.entities.append("unknown", validate="strict") # not bad, but its another reserved keyword argument thats hard to distinguish from other entity members

# vs

# 3
blueprint.entities.append("unknown")
blueprint.entities.validate()
# Or
blueprint.entities[-1].validate() # if you just want to validate the last one added

# If you wanted to absolutely make sure a newly added entity was validated beforehand, you could always do:

entity_to_add = new_entity(**args)
entity_to_add.validate().reissue_all()
blueprint.entities.append(entity_to_add, copy=False)

# 3 is best: it mirrors the design pattern used on parent objects (permissive construction, then strict validation) and it allows for the most user flexibility in the most apparent way

# However how do we handle overlapping entities? Thats one of the few cases where issuing a warning from the actual append would be better than getting a report after the fact
# Hence, we should use a hybrid; you can use #2 for inplace validation (on by default perhaps) and #3 for when you want more control for when to validate

# Fuck, but how do we handle:
blueprint.entities[0] = {"name": "some-entity"}

blueprint.entities.append(...) # validates
blueprint.entities.insert(...) # validates
blueprint.entities = [...] # validates? perhaps it should depend on blueprint.validate_assignment
blueprint.entities[...] = ... # probably also makes sense to have it based on blueprint.validate_assignment

# Maybe validate_assignment is the answer, just not as a member of `EntityList`. When we construct a Blueprint, it's one of the arguments after all:

blueprint = Blueprint(validate_assignment="none")

blueprint.entities = [{"name": "some-entity"}] # raw value
blueprint.entities[-1] = {"name": "some-other-entity"} # raw value
blueprint.version = "wrong" # raw value

blueprint.validate_assignment = True

# Continue with conversion
# ...


# Remember: we need to convert things into "internal" format whenever possible. This means that we need to "construct" any provided entity regardless of whether we validate it or not!
```

* Draftsman should probably be maximally concerned with correctness wherever possible
* However, it must also always provide a way to turn that correctness check off if the user desires it

Validating on construction is useful and ergonomic. Sometimes it's clearer to validate explicitly before or after (and you should be able to do that!) but in terms of default behavior I think just constructing an object should have the option to issue errors and warnings.

```py
blueprint = Blueprint(**busted_dict) # Issue
blueprint = Blueprint(**busted_dict, validate="none") # No issue

blueprint = Blueprint("wrong") # Issue, malformed blueprint string
blueprint = Blueprint("wrong", validate="none") # Issue, malformed blueprint string

blueprint = Blueprint("correct encoding, but wrong contents") # Issue
blueprint = Blueprint("correct encoding, but wrong contents", validate="none") # No issue

blueprint.load_from_string("correct encoding, but wrong contents") # Issue
blueprint.load_from_string("correct encoding, but wrong contents", validate="none") # No issue

# You can always post-validate once the object exists
result = blueprint.validate()
do_something_with(result)

blueprint.entities.append(**busted_dict) # Issue
blueprint.entities.append(**busted_dict, validate="none") # No issue

existing_entity = new_entity(..., validate="strict") # Can issue

blueprint.entities.append(existing_entity) # validates the entity twice
blueprint.entities.append(existing_entity, validate="none") # doesn't redo the work

blueprint.entities.validate_assignment = "strict"
blueprint.entities = ["wrong"] # Issue
blueprint.entities.validate_assignment = "none"
blueprint.entities = ["wrong"] # Still issue, entities are special case

busted_entity = new_entity(..., validate="none") # No issue

blueprint.entities.validate_assignment = "strict"
blueprint.entities[0] = busted_entity # Issue
blueprint.entities.validate_assignment = "none"
blueprint.entities[0] = busted_entity # No issue

# However, doing it the following way allows you to validate bad data only once:

bad_dict = {...}

blueprint.entities[0] = bad_dict # Coerced to entity 

# When you do an assignment to an EntityList, do you want to validate the entity you passed in?

# Maybe. Appending is often more ergonomic because you don't have to create a temporary first and then remember to use `copy=False` in order to save on operations, essentially acting like C++ `emplace`:
blueprint.entities.append(name="name", args=..., validate="strict") # So when constructing a new entity, we should typically validate it.

# However, if you're passing in an entity that already exists, it's less ergonmic to act as if the entity IS bad; if you wanted to validate bad data you were adding to a blueprint, you'd probably do:

potentially_bad_entity = new_entity(**unknown_dict)
potentially_bad_entity.validate(mode="strict").reissue_all()
blueprint.entities.append(potentially_bad_entity) # Here we would be validating twice. Is this good?

# Maybe not, on second glance. How does Draftsman know that `potentially_bad_entity` hasn't been modified to actually become invalid in the span between validating it and appending it? In this case it's not bad, but what if:

potentially_bad_entity = new_entity(**unknown_dict)
potentially_bad_entity.validate(mode="strict").reissue_all() # Good bill of health, no issues

potentially_bad_entity.name = 100 # Make an issue

blueprint.entities.append(potentially_bad_entity) # Here we would miss the obvious mistake if we didn't validate here

# Now, of course, the intermittent line would also issue an error under these circumstances. However, what should the preferred behavior of Draftsman be?
# Here, I would argue that consistency is king. If appending an entity from keyword arguments validates the entity by default, then adding a new entity by reference should also validate that entity.

# The problem gets more complex when we consider __setitem__. We have 2 options:

blueprint.entities[0] = some_entity_instance
blueprint.entities[0] = {"name": "some-entity-dict"}

# Unlike appending, we have no control over specifying whether we should validate the object beforehand.
# However, is that necessary? Instance case:

# We can check validation beforehand when we construct:
some_entity_instance = new_entity(..., validate="strict")
# Or even if we're given a premade, unvalidated object we can just revalidate it:
some_given_entity.validate().reissue_all()

blueprint.entities[0] = some_entity_instance
blueprint.entities[1] = some_given_entity

# It also seems to loosely align with the idea that "raw" dict access is unvalidated. You could argue that since we're using brackets here, this operation should not be validated.

```